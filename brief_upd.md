# Бриф PhotoChanger Platform

## Назначение платформы
PhotoChanger — серверная платформа для AI-обработки фотографий. Система принимает ingest-запросы от DSLR Remote Pro и по заранее
настроенным "слотам" маршрутизирует их к подходящему AI-провайдеру. Платформа удерживает синхронное соединение до наступления
конфигурируемого дедлайна `T_sync_response` (45–60 с) и либо возвращает результат в пределах окна ожидания, либо завершает запрос с 504, фиксируя статус задачи. Временные артефакты (ingest-файлы и публичные ссылки) по‑прежнему используют это значение в качестве TTL, а итоговые изображения, сохранённые  на диск, доступны 72 часа.

## Пользовательский workflow
1. Администратор авторизуется в веб-интерфейсе и попадает на главную страницу со списком из 15 статических слотов (`slot-001` … `slot-015`).
   В базовой поставке заданы два административных аккаунта (`serg`, `igor`) с правами полного доступа;
   последующая настройка пользователей выполняется отдельно и выходит за рамки текущего брифа.

2. При выборе слота открывается страница настроек: выбираются провайдер, операция и параметры. Интерфейс подстраивается под выбранную
   операцию, запрашивая необходимые шаблоны или временные загрузки.
3. Сохранённый слот отображает ingest-ссылку с кнопкой копирования. DSLR Remote Pro использует эту ссылку для отправки `POST` с
   изображением, паролем и дополнительными полями.
4. Статус выполнения и результат обработки доступны в интерфейсе после завершения задачи: страница слота показывает галерею последних изображений с превью и кнопками «Скачать» (доступ по публичной ссылке 72 часа). Повторный запуск выполняется тем же
   ingest-POST.
5. Отдельная страница статистики использует эндпоинты `/api/stats/global` и `/api/stats/{slot_id}` для отображения агрегатов по активным слотам и истории задач.

Подробные сценарии UI и взаимодействий описаны в [use-cases](../spec/docs/blueprints/use-cases.md) и [vision](../spec/docs/blueprints/vision.md).

## Цель платформы и общий процесс
Платформа синхронно обрабатывает ingest-запросы и управляет асинхронной очередью. Получив POST, API валидирует входные данные, рассчитывает дедлайны (`expires_at`, `T_ingest_ttl`, `T_public_link_ttl`, `T_result_retention`) и ожидает сформированный ответот корутины до наступления `T_sync_response`, выполняя polling записи `Job` в БД примерно раз в секунду без дополнительных механизмов уведомлений.

результат сохраняется в `MEDIA_ROOT/results`, путь фиксируется в `Job.result_file_path` вместе с метаданными (`result_mime_type`,
`result_size_bytes`, `result_checksum`, `result_expires_at = finalized_at + 72h`) и возвращается клиенту; при таймауте или ошибке
фиксируется `failure_reason`, а временные данные очищаются. Поведение очереди и дедлайнов подробно раскрыто в разделе "Контур дедлайнов" доменной модели.

Если провайдер Turbotext не возвращает результат до наступления `T_sync_response`, Ingest API отвечает 504 и немедленно завершает задачу.  Контракт ответа `components.responses.GatewayTimeout` в OpenAPI закрепляет, что Job финализирована и требует нового ingest-запроса для повторного запуска. При успешной обработке итоговый файл доступен по публичной ссылке `GET /public/results/{job_id}` до наступления `result_expires_at`.

## Провайдеры и операции
### Gemini
- Поддерживает генерацию, редактирование, слияние изображений на модели `gemini-2.5-flash-image` через `models.generateContent`.
- Работает с Files API и ограничивает форматы `image/png`, `image/jpeg`, `image/webp`, `image/heic`, `image/heif`.
- Ограничения по размеру (≤ 2 ГБ) и квоты (500 rps, 2 000 rpd) учитываются при валидации и планировании запросов.

### Turbotext
- Выполняет style-transfer, image-to-image и deepfake-сценарии через `api_ai` с поддержкой очереди и polling.
.

Детализированные настройки входов/выходов и маппинг полей находятся в [конфигурации провайдеров](../spec/docs/blueprints/domain-model.md#модели-данных-postgresql--alembic)
и соответствующих разделах [openapi-контракта](../spec/contracts/openapi.yaml).

## Медиа-хранилища и TTL
Платформа использует два типа хранилищ:
- **Временное публичное хранилище** (`media_object`) выдаёт ссылки, живущие `T_public_link_ttl = T_sync_response` секунд. Продление не
  предусмотрено: по истечении срока задача получает `failure_reason = 'timeout'`, данные удаляются, а для повторного использования файл регистрируется
  заново. Механизм описан в [constraints-risks](../spec/docs/blueprints/constraints-risks.md) и [test-plan](../spec/docs/blueprints/test-plan.md).
- **Постоянные шаблоны** (`template_media`) хранят эталонные изображения для слотов. Они управляются вручную и не очищаются автоматически.

- **Итоговые результаты** (`result_file_path`) сохраняются  в `MEDIA_ROOT/results` на 72 часа (`T_result_retention`). TTL результата не связан с `job.expires_at`: как только задача финализирована, файл доступен весь период `result_expires_at = finalized_at + 72h`. Очиститель удаляет файл по достижении `result_expires_at`, обнуляет путь и делает публичную ссылку недоступной (`410 Gone`). UI и внешние клиенты используют `GET /public/results/{job_id}` для скачивания.
  - Финальное решение **Issue 2** зафиксировало структуру каталога: результаты складываются в `MEDIA_ROOT/results/<job_id>.<ext>`, где расширение выводится из `result_mime_type`, а недостающие подпапки создаются автоматически.
  - Финальное решение **Issue 3** подтверждает фиксированное значение `T_result_retention = 72h` и необходимость сохранять контрольную сумму (`result_checksum`) вместе с метаданными результата.
  - Финальное решение **Issue 4** описывает фонового очистителя `photochanger-media-cleanup`, который запускается раз в 15 минут и вызывает `JobService.purge_expired_results` и `MediaService.purge_expired_media`, чтобы удалить просроченные файлы и обнулить публичные ссылки.

Итоговые результаты записываются в `Job.result_*`: базовая метаинформация (`result_mime_type`, `result_size_bytes`, `result_checksum`, `result_expires_at`) и путь к файлу. Base64-строка хранится только на время синхронного ожидания (очищается после отдачи HTTP 200/504).
Временные загрузки и шаблоны связываются со слотами через конфигурацию `settings_json`; детально — в [domain-model](../spec/docs/blueprints/domain-model.md).

## Управление настройками и секретами
Глобальные параметры (`T_sync_response`, TTLы медиа, признак наличия DSLR-пароля и ключей провайдеров) управляются через страницу настроек.
API позволяет читать агрегированное состояние (`GET /api/settings`) и обновлять значения (`PUT /api/settings`). Секреты хранятся в `app_settings`
и `provider_secret` с шифрованием и не возвращаются клиенту в явном виде. Кнопка очистки медиа кеша вызывает асинхронную задачу `POST /api/media/cache/purge`.
Контракты и форматы ответов определены в [OpenAPI](../spec/contracts/openapi.yaml); требования к безопасности изложены в [nfr](../spec/docs/blueprints/nfr.md).

## Архитектурные принципы
**1. Архитектура проекта ориентирована на безопасность изменений кода** при генерации кода LLM моделью: тонкие фасады + pure domain + contract-first.


**2. Архитектура проекта должна обладать Низкой связанностью (low coupling):**
 * Каждый модуль/фасад имеет отдельную ответственность.
 * Сервисы и домен не знают про инфраструктуру напрямую — используют фасады и интерфейсы.
 * Любой слой зависит только от более внутреннего слоя.

**3. Архитектура проекта должна обладать Высокой когезией (high cohesion):**
 * Каждый класс или модуль отвечает за один аспект.



## Архитектурный обзор
- **Ingest API** — валидирует запросы, рассчитывает дедлайны, ожидает синхронный ответ и завершает соединение строго по `T_sync_response`, выполняя чтение записи `Job` в БД с шагом ≈1 с на всём периоде ожидания.
- ** PostgreSQL** — таблица `job` выступает очередью с `SELECT … FOR UPDATE SKIP LOCKED`, обеспечивая back-pressure и контроль параллелизма.
- **UI и админ-панель** — управляют слотами, шаблонами и глобальными настройками, опираясь на статическую конфигурацию провайдеров (`configs/providers.json`) и получают статусы Job через API polling без webhook. Страница слота дополнительно показывает галерею `recent_results` (превью + кнопка «Скачать»), опираясь на публичный endpoint результатов.
- **Мониторинг и логирование** — фиксируют статусы ingest, таймауты, ретраи, операции очистки и циклы polling: чтение `Job` из БД со стороны Ingest API (шаг ≈1 с) и запросы `do=get_result` к провайдерам, задержки и долю задач с просроченным `queueid` в `ProcessingLog` и связанных метриках.

Диаграммы взаимодействий и ответственности компонентов приведены в [context](../spec/docs/blueprints/context.md) и [use-cases](../spec/docs/blueprints/use-cases.md).

## Ключевые сущности
- `Slot` — статический пул из 15 ingest-слотов (`slot-001` … `slot-015`), конфигурируемых администраторами.
- `Job` — запись очереди обработки: хранит статус, дедлайн (`expires_at`), ссылки на результат и `provider_job_reference`.
- `MediaObject` — временная публичная ссылка с жёстким TTL.
- `TemplateMedia` — долговечные шаблонные файлы.
- `ProcessingLog` — аудит ingest-запросов, включая статус, время ответа и ошибки.

Полная структура полей и инварианты описаны в [domain-model](../spec/docs/blueprints/domain-model.md) и согласованы с [openapi.yaml](../spec/contracts/openapi.yaml).

## Работа с провайдерами
- Для асинхронных сценариев Turbotext сохраняется `queueid`; приложение опрашивает `do=get_result` только в пределах `T_sync_response`. По истечении дедлайна задача завершается с `failure_reason = 'timeout'`, дальнейший polling не выполняется.
- Gemini получает бинарные данные напрямую или через Files API; обработанные изображения скачиваются и сохраняются в `Job.result_*`.

Специфика интеграций и лимиты операций перечислены в [providers configuration](../spec/docs/blueprints/domain-model.md#провайдеры-и-операции) и [nfr](../spec/docs/blueprints/nfr.md).

Этот бриф остаётся источником повествовательного контекста и дополняет формальные спецификации, определяющие контракты и реализацию.
