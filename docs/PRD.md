# PhotoChanger Platform — Iteration 2 Delivery Brief

## 1. Обзор продукта
- **Назначение.** PhotoChanger обрабатывает фотографии, поступающие из DSLR Remote Pro, через выбранных AI-провайдеров (Gemini, Turbotext) и гарантирует ответ в пределах окна ожидания 10–60 секунд.
- **Ценность.** Администраторы студии получают управляемые слоты и прозрачную статистику, а операторы — быстрый результат без ручного контакта с техподдержкой.
- **KPI итерации.**
  - ≥ 95 % ingest-запросов завершаются HTTP 200 в пределах `T_sync_response`.
  - ≤ 5 % запросов завершается 504 (таймаут провайдера).
  - Итоговые файлы доступны 72 часа, после чего автоматически удаляются.

## 2. Стейкхолдеры и роли
| Роль | Ответственность | Основные интерфейсы |
|------|-----------------|---------------------|
| **Администраторы** (предсозданные аккаунты `serg`, `igor`) | Настройка 15 слотов, управление глобальными параметрами, мониторинг статистики, ручная очистка медиа | Админ UI, REST API `/api/admin/*`, cron-скрипты |
| **Операторы DSLR Remote Pro** | Отправка фотографий на обработку, скачивание результатов | Публичные ingest-ссылки, страница слота с галереей |
| **AI-провайдеры** (Gemini, Turbotext) | Предоставление сервиса обработки | Внешние HTTP API через драйверы |
| **Ops-команда** | Деплой, управление секретами, мониторинг, реагирование на инциденты | CI/CD, `.env`, Prometheus, cron |

## 3. Цели и границы итерации
### В зоне ответственности
- Поддержка 15 статических слотов с настройкой провайдера, шаблонов, лимитов размера и ingest-пароля.
- Синхронная обработка ingest-запросов без очередей и фоновых воркеров.
- Управление временным и постоянным хранением медиа с TTL и ручной очисткой.
- Статистика по слотам и глобально (активные задачи, доля ошибок, последние результаты).
- Health-check, базовая наблюдаемость (логи, метрики, алерты).
- Документация контрактов (`spec/contracts/*`) и UI сценариев (`spec/docs/blueprints/*`).

### Вне итерации (не реализовывать)
- Подключение новых провайдеров сверх Gemini/Turbotext.
- Расширенная RBAC, self-service админов, SSO.
- Очереди задач, фоновые воркеры, распределённое хранилище.
- Долгосрочное архивирование результатов > 72 часов.

## 4. Ключевые требования
### Функциональные
1. Ingest-endpoint `POST /api/ingest/{slot_id}` принимает multipart-файл + метаданные, проверяет пароль слота и возвращает результат или таймаут.
2. `GET /api/jobs/{job_id}` позволяет оператору отслеживать статус в течение TTL.
3. Админ UI обеспечивает CRUD настроек слотов, обновление глобальных параметров (`T_sync_response`, TTL, пароли ingest), просмотр статистики и галереи результатов.
4. Галерея слота отображает последние N (по умолчанию 10) результатов с превью и ссылкой на скачивание.
5. Публичные ссылки `/public/results/{job_id}` действуют 72 часа и возвращают `410 Gone` после истечения срока.
6. Cron-скрипт `scripts/cleanup_media.py` удаляет просроченные файлы и помечает записи в БД.

### Нефункциональные
- Среднее время ответа успешного ingest ≤ 5 секунд при размере файла до 50 МБ.
- Таймаут провайдера ограничен `T_sync_response`; при превышении фиксируется статус `timeout` без повторов.
- Доступность API и UI ≥ 99 % в рабочие часы (08:00–22:00 MSK).
- Логи и метрики доступны в течение 30 дней.
- Критические ошибки (500/504 > 10 % за 15 минут) приводят к алерту ops-команды.
- Ограничения нагрузки:
  - Глобальный rate limit для POST /api/ingest: X rps, burst Y.
  - Конкурентность per-slot ≤ N (конфиг), per-provider ≤ M (конфиг).
- Идемпотентность:
  - Поддержка Idempotency-Key; дубликаты по хэшу файла возвращают ранее выданный job_id.

## 5. Пользовательские сценарии
### Администратор
1. Авторизуется (JWT, статические аккаунты из `secrets/runtime_credentials.json`).
2. Просматривает список слотов (ID, провайдер, статус, шаблон, лимит размера).
3. Редактирует слот: меняет провайдера, параметры, ingest-пароль, шаблоны.
4. Получает ingest-URL и передаёт операторам.
5. Открывает страницу статистики: фильтр по слотам, метрики успехов/ошибок, последние результаты.
6. Запускает ручную очистку медиа при необходимости.

### Оператор DSLR Remote Pro
1. Копирует ingest-URL.
2. Отправляет изображение через `POST /api/ingest/{slot_id}` с паролем.
3. Получает ответ: 200 (результат + публичная ссылка) или 504/5xx с объяснением.
4. Скачивает результат или повторяет запрос при необходимости.
5. Открывает публичную страницу слота для просмотра истории результатов.

### Ops-команда
1. Настраивает `.env` (секреты провайдеров, JWT-подпись, настройки БД).
2. Деплоит приложение (Docker/VM) и запускает cron каждые 15 минут.
3. Настраивает мониторинг Prometheus и алерты.
4. Ротирует секреты провайдеров по регламенту.

## 6. Архитектура и компоненты
### Высокоуровневый контур
```
DSLR Remote Pro -> FastAPI (ingest) -> Provider drivers (Gemini/Turbotext)
                                   -> PostgreSQL (slots, job_history, media_object)
                                   -> File system (media/temp, media/results)
                                   -> Admin UI / Public gallery
```

### Приложение FastAPI
- Один процесс, uvicorn workers = 1–2 (горизонтальное масштабирование не требуется на этой итерации).
- Конфигурация через `AppConfig` в `app/main.py`, зависимости передаются через `Depends` без контейнера внедрения.
- Вертикальные модули: `ingest`, `media`, `slots`, `settings`, `stats` (каждый со своими сервисами, схемами, тестами, README).
- `IngestService` валидирует запросы, кладёт исходный файл в `TempMediaStore`, вызывает драйвер провайдера внутри `asyncio.wait_for(..., timeout=T_sync_response)` и сохраняет результат в `ResultStore`. Незавершённые задачи не восстанавливаются после рестарта — статус фиксируется в `job_history`.
- `MediaService` и связанные фасады проверяют TTL на чтении и удаляют просроченные файлы лениво, чтобы придерживаться KISS-подхода без фоновых воркеров.
- `StatsService` агрегирует метрики по слотам и по системе из таблиц `job_history` и `slot`, отдаёт REST-ответы без дополнительного кэширования.

### Поток обработки ingest-запроса
1. Оператор отправляет `POST /api/ingest/{slot_id}` с multipart-файлом, паролем и метаданными.
2. Контроллер создаёт `JobContext`, валидирует размер/MIME, сохраняет файл во временный каталог `media/temp/{slot_id}/{job_id}` через `TempMediaStore`.
3. Сервис подбирает `ProviderDriver` по настройкам слота и вызывает `driver.process(job_ctx)` в `asyncio.wait_for` с таймаутом `T_sync_response`.
4. При успешном ответе драйвера файл переносится в `ResultStore`, запись `job_history` обновляется статусом `done`, `result_expires_at = now + 72h`, формируется ссылка `/public/results/{job_id}`.
5. Если `asyncio.TimeoutError` или исключение провайдера, сервис фиксирует `status='timeout'/'failed'`, удаляет временные файлы и возвращает 504 или 5xx.
6. Клиент может опрашивать `GET /api/jobs/{job_id}` до истечения TTL; поздние ответы провайдера игнорируются, потому что корутина отменяется `wait_for`.

### Хранилища
- **PostgreSQL** (миграции в `alembic/`):
  - `slot(id, provider_id, template_id, size_limit_mb, ingest_password_hash, settings_json)`.
  - `job_history(job_id, slot_id, status, provider_id, started_at, finished_at, duration_ms, failure_reason, result_path, result_expires_at)`.
  - `media_object(id, job_id, slot_id, type, path, expires_at, cleaned_at)`.
  - `settings(key, value, version, updated_at, updated_by)`.
- **Файловая система**:
  - `media/temp/{slot_id}/{job_id}/payload.*` (TTL = `T_sync_response`).
  - `media/results/{job_id}.{ext}` (TTL = 72 часа) + необязательные превью `*_preview.*`.
- TTL контролируется в сервисах: при обращении к записи с `expires_at < now` файл удаляется и помечается очищенным в БД; планировщик не используется.

### Провайдеры
- Общий интерфейс `ProviderDriver.process(job_ctx) -> ProviderResult`; реализации отвечают за сетевые вызовы и конвертацию форматов.
- `GeminiDriver` — асинхронный HTTP-клиент на `httpx.AsyncClient`, соблюдает ограничения сервиса по форматам/размерам и возвращает локальный путь или байтовые данные результата.
- `TurbotextDriver` — асинхронный клиент с polling внутри одной корутины, ограниченный `asyncio.wait_for` на уровне сервиса; повторных попыток нет.
- Таймауты HTTP-клиентов задаются в драйверах, `IngestService` дополнительно ограничивает длительность вызова через `asyncio.wait_for`.
- Настройка доступных драйверов и ключей провайдеров через `AppConfig` и `.env`.

## 7. API-слой и контракты
### Основные публичные эндпоинты
| Method & Path | Назначение | Ключевые статусы |
|---------------|------------|------------------|
| `POST /api/ingest/{slot_id}` | Приём файла, запуск обработки | 200 (успех), 400 (валидация), 401 (неверный пароль), 404 (слот не найден), 504 (таймаут провайдера), 5xx (ошибка провайдера) |
| `GET /api/jobs/{job_id}` | Проверка статуса обработки | 200 (объект статуса), 404 (job не найден/истёк TTL) |
| `GET /public/slots/{slot_id}` | Публичная галерея результатов | 200, 404 |
| `GET /public/results/{job_id}` | Скачивание результата | 200 (файл), 404 (не существует), 410 (TTL истёк) |

### Админ-API
- `POST /api/login` — получение JWT.
- `GET/PUT /api/settings` — чтение и обновление глобальных параметров.
- `GET/POST/PUT /api/slots` — управление слотами.
- `POST /api/slots/{slot_id}/cleanup` — ручная очистка медиа.
- `GET /api/stats/slots` и `GET /api/stats/system` — статистика.

Полные схемы и примеры ответов зафиксированы в `spec/contracts/openapi.yaml`. Любые изменения контрактов требуют обновления спецификаций и SemVer в `spec/contracts/VERSION.json`.

## 8. UI и UX
- **Технология.** Лёгкий SPA или статические страницы в `frontend/`, взаимодействуют с REST API.
- **Страницы:**
  - Логин (формы: email/пароль, отображение ошибок 401/429, блокировка после N попыток).
  - Дашборд слотов: таблица 15 слотов, индикаторы провайдера, статус активности, кнопки «Редактировать»/«Скопировать ссылку».
  - Форма редактирования слота: выбор провайдера, шаблоны, лимиты, пароль, превью ingest-URL.
  - Глобальные настройки: поля `T_sync_response`, TTL результатов, флаги очистки, список администраторов (read-only).
  - Статистика: графики p95 времени, доля 504, таблица последних задач.
  - Галерея слота (публичная): превью изображений, кнопка скачивания, отметка истечения TTL.
- **UI-правила:**
  - Все критические действия подтверждаются модалками.
  - Формы валидируются до отправки (формат файла, лимит размера, обязательные поля).
  - Состояния загрузки/ошибок отображаются явно; ошибки из API показываются локализованными сообщениями.

## 9. Безопасность и соответствие
- JWT авторизация админов, хэши паролей в `secrets/runtime_credentials.json`.
- Пароли ingest хранятся как bcrypt-хэши в таблице `slot`.
- Секреты провайдеров и JWT-подпись загружаются из `.env`/переменных окружения; файл `.env` не коммитится.
- Логи не содержат персональных данных и бинарных payload.
- Ограничение размера загрузки: по умолчанию 50 МБ (конфигурируется), жёсткий предел FastAPI 2 ГБ.
- Throttling логина: блокировка после 5 неудачных попыток на 15 минут.
- Все внешние вызовы выполняются по HTTPS, валидация TLS обязательна.

## 10. Операции и DevOps
- **Развёртывание:** Docker-контейнер с FastAPI, `uvicorn`, Python ≥ 3.11. PostgreSQL 15, общий том для `media/`.
- **Конфигурация окружения:**
  - `DATABASE_URL`
  - `MEDIA_ROOT`
  - `RESULT_TTL_HOURS` (72)
  - `TEMP_TTL_SECONDS` (`T_sync_response`)
  - `JWT_SIGNING_KEY`
  - `GEMINI_API_KEY`, `TURBOTEXT_API_KEY`
- **Миграции:** `alembic upgrade head` при деплое.
- **Cron-очистка:** `python scripts/cleanup_media.py` каждые 15 минут; логирование результатов в syslog.
- **Мониторинг:**
  - `GET /metrics` (Prometheus) — p95 времени ingest, доля 504, объём каталогов `media/*`.
  - Алертинг через Prometheus Alertmanager (пороговые значения KPI).
- **Наблюдаемость:** структурированные логи через `structlog` (ingest-ивенты, ошибки провайдеров, очистка медиа), `GET /healthz` проверяет подключение к PostgreSQL, наличие путей файловой системы и доступность конечных точек провайдеров через лёгкие ping-запросы.
- **Бэкапы:** ежедневный dump PostgreSQL, снапшоты файлов `media/results` (необязательно, учитывая TTL, но минимум на время релиза).

## 11. Тестирование и качество
- Unit-тесты для сервисов модулей и драйверов провайдеров (pytest, фейковые адаптеры).
- Интеграционные тесты FastAPI с временными каталогами и PostgreSQL (pytest-postgresql).
- Контрактные тесты провайдеров с мок-серверами (Gemini, Turbotext).
- Тесты скриптов (`scripts/cleanup_media.py`).
- UI-тесты smoke уровня (Playwright или Cypress) на сценарии логина, редактирования слота, просмотра статистики.
- Обязательный прогон линтеров/форматтеров (`ruff`, `black`, `mypy`) перед релизом.

## 12. План поставки и контрольные точки
| Этап | Содержание | Проверка |
|------|------------|----------|
| **E1. Аналитика и дизайн** | Уточнение требований, согласование API/датамодели, обновление ADR/контрактов | Подписанный бриф, ADR при необходимости |
| **E2. Бэкенд** | Реализация модулей `ingest`, `media`, `slots`, `settings`, `stats`, драйверов провайдеров | Unit + интеграционные тесты, покрытие KPI таймаутов |
| **E3. UI** | Разработка админ-интерфейса и публичной галереи | UI smoke-тесты, UX-ревью |
| **E4. Ops** | Сборка Docker-образа, настройка БД, cron, мониторинга | Успешный деплой на staging, проход health-check |
| **E5. Приёмка** | Совместные проверки SLA, таймаутов, очистки, security чеклист | Подписанный акт приёмки |

## 13. Критерии приёмки
- Контракты API соответствуют `spec/contracts/openapi.yaml`, SemVer обновлён.
- Интеграционный тест «успешный ingest» завершается < `T_sync_response` с получением результата и публикацией ссылки.
- Сценарий таймаута провайдера возвращает 504, статус `timeout` записан в `job_history`.
- Cron-очистка удаляет просроченные файлы и помечает `media_object.cleaned_at`.
- Галерея слота отображает последние 10 результатов и корректно скрывает просроченные записи.
- Логин админа, редактирование слота, обновление настроек проходят с JWT-проверкой и валидацией прав.
- Health-check `GET /healthz` проверяет PostgreSQL, наличие директорий медиа, доступность провайдеров.

## 14. Риски и смягчающие меры
| Риск | Вероятность | Влияние | Митигирующая мера |
|------|-------------|---------|-------------------|
| Нестабильность провайдеров (долгие ответы) | Средняя | Высокое (рост 504) | Чёткие таймауты, логирование, алерты, возможность быстрого переключения слота на другого провайдера |
| Заполнение диска `media/` | Средняя | Среднее | Мониторинг размера каталогов, cron-очистка, ручной запуск очистки из UI |
| Потеря секретов провайдеров | Низкая | Высокое | Секреты вне репозитория, ротация, ограничение прав доступа |
| Ошибки конфигурации слотов | Средняя | Среднее | Валидация UI/API, шаблоны конфигураций, журнал изменений |
| Человеческий фактор в Ops | Низкая | Среднее | Чеклисты деплоя, автоматизация через CI/CD |

## 15. Будущие шаги (post-iteration roadmap)
- Добавление новых провайдеров (Midjourney, Stable Diffusion) через тот же интерфейс драйверов.
- Расширенный RBAC (динамические роли, аудиты изменений).
- Геораспределённое хранение результатов и CDN.
- Асинхронная очередь для долгих задач (> 60 секунд) с повторными попытками.

## 16. Ссылки и артефакты
- Архитектурные решения: `spec/adr/` (см. индекс в `.memory/DECISIONS.md`).
- Контракты API и модели: `spec/contracts/openapi.yaml`, `spec/contracts/VERSION.json`.
- Блюпринты UI и user-flow: `spec/docs/blueprints/*`.
- История задач и решений: `.memory/TASKS.md`, `.memory/WORKLOG.md`, `.memory/ASKS.md`.

Документ одобрен тимлидом и служит основой для разработки итерации 2. Любые изменения требуют согласования и обновления брифа.