#!/usr/bin/env python3
"""Generate HTTP handler stubs and Pydantic DTOs from the contracts.

Usage
-----
Run ``python scripts/gen_stubs.py`` to read ``spec/contracts/openapi.yaml``
and JSON Schemas from ``spec/contracts/schemas``. The script regenerates
``src/app/api/generated/stubs.py`` and ``src/app/api/schemas/models.py``
using lightweight Jinja2 templates. PyYAML and Jinja2 must be available in
the environment.
"""

from __future__ import annotations

import argparse
import json
import keyword
import re
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Iterable, Mapping, Sequence

try:
    from jinja2 import Environment, StrictUndefined
except ImportError as exc:  # pragma: no cover - dependency guard
    raise SystemExit(
        "Jinja2 is required to generate scaffolding. Install it with `pip install jinja2`."
    ) from exc

try:
    from yaml import safe_load  # type: ignore[import-untyped]
except ImportError as exc:  # pragma: no cover - dependency guard
    raise SystemExit(
        "PyYAML is required to generate scaffolding. Install it with `pip install pyyaml`."
    ) from exc

STUBS_TEMPLATE = """
\"\"\"Auto-generated API handler stubs.

Generated by scripts/gen_stubs.py.
Source contract version: {{ version }}
Do not edit manually â€” regenerate the file after changing the contracts.
\"\"\"

from __future__ import annotations

from typing import Any, Callable, Dict, Tuple

__all__ = ["ROUTE_HANDLERS"]

{% for op in operations %}

def {{ op.function_name }}(*args: Any, **kwargs: Any) -> None:
    \"\"\"{{ op.docstring | replace('\n', '\n    ') }}\"\"\"
    raise NotImplementedError("Implement handler for {{ op.method }} {{ op.path }}")
{% endfor %}

ROUTE_HANDLERS: Dict[Tuple[str, str], Callable[..., Any]] = {
{% for op in operations %}
    ("{{ op.method }}", "{{ op.path }}"): {{ op.function_name }},
{% endfor %}
}
"""

MODELS_TEMPLATE = """
\"\"\"Auto-generated Pydantic models derived from spec/contracts/schemas.

Generated by scripts/gen_stubs.py. Do not edit manually.
\"\"\"

from __future__ import annotations

{% for line in import_lines %}{{ line }}
{% endfor %}

__all__ = [
{% for alias in aliases %}    "{{ alias.name }}",
{% endfor %}
{% for model in models %}    "{{ model.name }}",
{% endfor %}]

{% for alias in aliases %}
{{ alias.name }} = {{ alias.expression }}
{% if alias.docstring %}\"\"\"{{ alias.docstring | replace('\n', '\n') }}\"\"\"{% endif %}

{% endfor %}
{% for model in models %}

class {{ model.name }}(BaseModel):
    \"\"\"{{ model.docstring | replace('\n', '\n    ') }}\"\"\"
{% if model.extra %}    model_config = ConfigDict(extra="{{ model.extra }}")
{% endif %}{% for field in model.fields %}    {{ field.name }}: {{ field.annotation }} = {{ field.field_expression }}
{% endfor %}
{% endfor %}
"""

SUPPORTED_METHODS = {"get", "put", "post", "delete", "patch", "options", "head"}

COMPAT_ALIAS_OVERRIDES = {"SlotId": "SlotIdentifier"}


@dataclass(slots=True)
class Operation:
    method: str
    path: str
    summary: str
    description: str

    @property
    def function_name(self) -> str:
        return f"{self.method.lower()}_{_normalise_path(self.path)}"

    @property
    def docstring(self) -> str:
        summary = self.summary.strip()
        description = self.description.strip()
        if summary and description:
            text = f"{summary}\n\n{description}"
        else:
            text = (
                summary
                or description
                or f"Handler for {self.method.upper()} {self.path}."
            )
        return text.replace('"""', '"""')


def _normalise_path(path: str) -> str:
    parts: list[str] = []
    for raw in path.strip("/").split("/"):
        if not raw:
            continue
        if raw.startswith("{") and raw.endswith("}"):
            raw = f"by_{raw[1:-1]}"
        safe = re.sub(r"[^a-zA-Z0-9_]", "_", raw).strip("_")
        parts.append(safe.lower())
    return "_".join(parts) or "root"


@dataclass(slots=True)
class ModelField:
    name: str
    annotation: str
    field_expression: str


@dataclass(slots=True)
class ModelSpec:
    name: str
    docstring: str
    fields: list[ModelField]
    extra: str | None = None


@dataclass(slots=True)
class AliasSpec:
    name: str
    expression: str
    docstring: str | None
    imports: set[tuple[str, str]] = field(default_factory=set)


@dataclass(slots=True)
class GeneratedModels:
    models: list[ModelSpec]
    aliases: list[AliasSpec]
    import_lines: list[str]


@dataclass(slots=True)
class TypeInfo:
    annotation: str
    imports: set[tuple[str, str]] = field(default_factory=set)
    extra_models: list[ModelSpec] = field(default_factory=list)


class SchemaGenerator:
    def __init__(self, root: Path) -> None:
        self.root = root

    def generate(
        self, *, components: Mapping[str, Any] | None = None
    ) -> GeneratedModels:
        models: list[ModelSpec] = []
        imports: set[tuple[str, str]] = {
            ("pydantic", "BaseModel"),
            ("pydantic", "Field"),
        }
        config_needed = False
        aliases: list[AliasSpec] = []
        for schema_path in sorted(self.root.glob("*.json")):
            schema = json.loads(schema_path.read_text(encoding="utf-8"))
            model, extras, model_imports, needs_config = self._build_model(
                schema, inline_name=schema_path.stem
            )
            models.append(model)
            models.extend(extras)
            imports |= model_imports
            if any(extra.extra for extra in extras):
                config_needed = True
            config_needed = config_needed or needs_config
        if config_needed:
            imports.add(("pydantic", "ConfigDict"))
        if components:
            alias_specs, alias_imports = self._build_aliases(components)
            aliases.extend(alias_specs)
            imports |= alias_imports
        import_lines = _format_imports(imports)
        return GeneratedModels(
            models=models, aliases=aliases, import_lines=import_lines
        )

    def _build_model(
        self,
        schema: Mapping[str, Any],
        *,
        inline_name: str | None = None,
    ) -> tuple[ModelSpec, list[ModelSpec], set[tuple[str, str]], bool]:
        name_hint = self._preferred_name(inline_name, schema)
        description = str(schema.get("description", f"Schema for {name_hint}"))
        additional = schema.get("additionalProperties")
        extra_mode: str | None = None
        if additional is False:
            extra_mode = "forbid"
        elif additional is True:
            extra_mode = "allow"
        elif isinstance(additional, Mapping):
            extra_mode = "allow"
            description = (
                f"{description}\n\nAdditional properties follow schema-defined types."
            )

        fields: list[ModelField] = []
        imports: set[tuple[str, str]] = set()
        nested_models: list[ModelSpec] = []
        required = set(schema.get("required", []))
        for raw_name, prop_schema in (schema.get("properties") or {}).items():
            field_name, alias = _normalise_field_name(raw_name)
            type_info = self._resolve_type(f"{name_hint}_{raw_name}", prop_schema)
            annotation = type_info.annotation
            if raw_name not in required and "None" not in annotation:
                annotation = f"{annotation} | None"
            default_token = "..." if raw_name in required else "None"
            field_args = _collect_field_args(prop_schema, alias)
            field_expression = _format_field_expression(default_token, field_args)
            fields.append(
                ModelField(
                    name=field_name,
                    annotation=annotation,
                    field_expression=field_expression,
                )
            )
            imports |= type_info.imports
            nested_models.extend(type_info.extra_models)
        docstring = description.replace('"""', '"""')
        model = ModelSpec(
            name=_to_pascal_case(name_hint),
            docstring=docstring,
            fields=fields,
            extra=extra_mode,
        )
        return model, nested_models, imports, extra_mode is not None

    def _preferred_name(
        self, inline_name: str | None, schema: Mapping[str, Any]
    ) -> str:
        inline = inline_name or ""
        raw_title = schema.get("title")
        title = str(raw_title) if isinstance(raw_title, str) and raw_title else ""
        if inline:
            if inline.endswith(("Request", "Response")):
                return inline
            if title and inline.lower() == title.lower():
                return inline
            if not title:
                return inline
        if title:
            return title
        if inline:
            return inline
        return "AnonymousModel"

    def _build_aliases(
        self, components: Mapping[str, Any]
    ) -> tuple[list[AliasSpec], set[tuple[str, str]]]:
        aliases: list[AliasSpec] = []
        imports: set[tuple[str, str]] = set()
        for name in sorted(components):
            schema = components[name]
            alias = self._build_alias(name, schema)
            if alias is None:
                continue
            aliases.append(alias)
            imports |= alias.imports
        compat_aliases = []
        for original, alias_name in COMPAT_ALIAS_OVERRIDES.items():
            target_name = _to_pascal_case(original)
            match = next((item for item in aliases if item.name == target_name), None)
            if match is None:
                continue
            compat_aliases.append(
                AliasSpec(
                    name=alias_name,
                    expression=match.name,
                    docstring=(f"Backwards compatible alias for {match.name}."),
                )
            )
        aliases.extend(compat_aliases)
        return aliases, imports

    def _build_alias(self, name: str, schema: Mapping[str, Any]) -> AliasSpec | None:
        if not isinstance(schema, Mapping):
            return None
        if "enum" not in schema and "const" not in schema:
            return None
        type_info = self._resolve_type(name, schema)
        description = str(schema.get("description", f"Alias for {name}"))
        return AliasSpec(
            name=_to_pascal_case(name),
            expression=type_info.annotation,
            docstring=description.replace('"""', '"""'),
            imports=type_info.imports,
        )

    def _resolve_type(self, context: str, schema: Mapping[str, Any]) -> TypeInfo:
        if "$ref" in schema:
            ref_name = Path(schema["$ref"]).stem
            return TypeInfo(annotation=_to_pascal_case(ref_name))
        if "enum" in schema:
            values = ", ".join(_literal(value) for value in schema["enum"])
            return TypeInfo(
                annotation=f"Literal[{values}]", imports={("typing", "Literal")}
            )
        if "const" in schema:
            value = _literal(schema["const"])
            return TypeInfo(
                annotation=f"Literal[{value}]", imports={("typing", "Literal")}
            )
        if "oneOf" in schema:
            return self._resolve_union(context, schema["oneOf"])
        if "anyOf" in schema:
            return self._resolve_union(context, schema["anyOf"])
        type_value = schema.get("type")
        if isinstance(type_value, list):
            merged = []
            for subtype in type_value:
                merged.append(self._resolve_type(context, {**schema, "type": subtype}))
            return _merge_union(merged)
        if type_value == "null":
            return TypeInfo(annotation="None")
        if type_value == "string":
            fmt = schema.get("format")
            if fmt == "date-time":
                return TypeInfo(
                    annotation="datetime", imports={("datetime", "datetime")}
                )
            if fmt == "date":
                return TypeInfo(annotation="date", imports={("datetime", "date")})
            if fmt == "uuid":
                return TypeInfo(annotation="UUID", imports={("uuid", "UUID")})
            return TypeInfo(annotation="str")
        if type_value == "integer":
            return TypeInfo(annotation="int")
        if type_value == "number":
            return TypeInfo(annotation="float")
        if type_value == "boolean":
            return TypeInfo(annotation="bool")
        if type_value == "array":
            items = schema.get("items", {"type": "string"})
            item_info = self._resolve_type(f"{context}Item", items)
            annotation = f"list[{item_info.annotation}]"
            return TypeInfo(
                annotation=annotation,
                imports=item_info.imports,
                extra_models=item_info.extra_models,
            )
        if type_value == "object":
            properties = schema.get("properties")
            additional = schema.get("additionalProperties")
            if properties:
                inline_name = _to_pascal_case(context)
                inline_model, extras, imports, _ = self._build_model(
                    schema, inline_name=inline_name
                )
                all_models = [inline_model, *extras]
                return TypeInfo(
                    annotation=inline_model.name,
                    imports=imports,
                    extra_models=all_models,
                )
            if additional is True:
                return TypeInfo(
                    annotation="dict[str, Any]", imports={("typing", "Any")}
                )
            if additional is False:
                return TypeInfo(annotation="dict[str, Any]")
            if isinstance(additional, Mapping):
                value_info = self._resolve_type(f"{context}Value", additional)
                imports = (
                    value_info.imports | {("typing", "Any")}
                    if value_info.annotation == "Any"
                    else value_info.imports
                )
                annotation = f"dict[str, {value_info.annotation}]"
                return TypeInfo(
                    annotation=annotation,
                    imports=imports,
                    extra_models=value_info.extra_models,
                )
        return TypeInfo(annotation="Any", imports={("typing", "Any")})

    def _resolve_union(
        self, context: str, schemas: Sequence[Mapping[str, Any]]
    ) -> TypeInfo:
        members: list[TypeInfo] = []
        has_none = False
        for option in schemas:
            if option.get("type") == "null":
                has_none = True
                continue
            members.append(self._resolve_type(context, option))
        merged = _merge_union(members)
        if has_none and "None" not in merged.annotation:
            merged.annotation = f"{merged.annotation} | None"
        return merged


def _merge_union(options: Iterable[TypeInfo]) -> TypeInfo:
    annotations: list[str] = []
    imports: set[tuple[str, str]] = set()
    extra_models: list[ModelSpec] = []
    for option in options:
        for model in option.extra_models:
            extra_models.append(model)
        imports |= option.imports
        annotations.append(option.annotation)
    if not annotations:
        return TypeInfo(annotation="Any", imports={("typing", "Any")})
    deduped = list(dict.fromkeys(annotations))
    annotation = " | ".join(deduped)
    return TypeInfo(annotation=annotation, imports=imports, extra_models=extra_models)


def _collect_field_args(schema: Mapping[str, Any], alias: str | None) -> dict[str, Any]:
    args: dict[str, Any] = {}
    if alias is not None:
        args["alias"] = alias
    if "description" in schema and schema["description"]:
        args["description"] = schema["description"]
    if "minLength" in schema:
        args["min_length"] = schema["minLength"]
    if "maxLength" in schema:
        args["max_length"] = schema["maxLength"]
    if "pattern" in schema:
        args["pattern"] = schema["pattern"]
    if "minimum" in schema:
        args["ge"] = schema["minimum"]
    if "maximum" in schema:
        args["le"] = schema["maximum"]
    if "exclusiveMinimum" in schema:
        args["gt"] = schema["exclusiveMinimum"]
    if "exclusiveMaximum" in schema:
        args["lt"] = schema["exclusiveMaximum"]
    if "minItems" in schema:
        args["min_items"] = schema["minItems"]
    if "maxItems" in schema:
        args["max_items"] = schema["maxItems"]
    return args


def _format_field_expression(default_token: str, args: Mapping[str, Any]) -> str:
    if not args:
        return f"Field({default_token})"
    rendered = ", ".join(
        f"{key}={json.dumps(value, ensure_ascii=False)}" for key, value in args.items()
    )
    return f"Field({default_token}, {rendered})"


def _literal(value: Any) -> str:
    if isinstance(value, str):
        return repr(value)
    if isinstance(value, bool):
        return "True" if value else "False"
    if value is None:
        return "None"
    return str(value)


def _format_imports(imports: set[tuple[str, str]]) -> list[str]:
    grouped: dict[str, set[str]] = {}
    for module, name in imports:
        grouped.setdefault(module, set()).add(name)
    lines: list[str] = []
    for module in sorted(grouped):
        names = ", ".join(sorted(grouped[module]))
        lines.append(f"from {module} import {names}")
    return lines


def _to_pascal_case(value: str) -> str:
    parts = re.split(r"[^0-9a-zA-Z]+", value)
    tokens: list[str] = []
    for part in parts:
        if not part:
            continue
        if part[:1].isalpha():
            tokens.append(part[:1].upper() + part[1:])
        else:
            tokens.append(part)
    return "".join(tokens) or "Model"


def _normalise_field_name(name: str) -> tuple[str, str | None]:
    candidate = re.sub(r"[^0-9a-zA-Z_]+", "_", name)
    if not candidate:
        candidate = "field"
    if candidate[0].isdigit():
        candidate = f"field_{candidate}"
    alias: str | None = None
    if candidate != name or not candidate.isidentifier():
        alias = name
    if keyword.iskeyword(candidate) or not candidate.isidentifier():
        candidate = f"{candidate}_"
        alias = name
    return candidate, alias


def _parse_operations(spec: Mapping[str, Any]) -> list[Operation]:
    operations: list[Operation] = []
    for path, path_item in spec.get("paths", {}).items():
        if not isinstance(path_item, Mapping):
            continue
        for method, raw_op in path_item.items():
            if method not in SUPPORTED_METHODS:
                continue
            if not isinstance(raw_op, Mapping):
                continue
            operations.append(
                Operation(
                    method=method.upper(),
                    path=str(path),
                    summary=str(raw_op.get("summary", "")),
                    description=str(raw_op.get("description", "")),
                )
            )
    operations.sort(key=lambda op: (op.path, op.method))
    return operations


def render_stubs(operations: Sequence[Operation], version: str) -> str:
    env = Environment(trim_blocks=True, lstrip_blocks=True, undefined=StrictUndefined)
    template = env.from_string(STUBS_TEMPLATE)
    return template.render(operations=operations, version=version).strip() + "\n"


def render_models(models: GeneratedModels) -> str:
    env = Environment(trim_blocks=True, lstrip_blocks=True, undefined=StrictUndefined)
    template = env.from_string(MODELS_TEMPLATE)
    return (
        template.render(
            models=models.models,
            aliases=models.aliases,
            import_lines=models.import_lines,
        ).strip()
        + "\n"
    )


def main() -> None:
    parser = argparse.ArgumentParser(description=__doc__)
    root = Path(__file__).resolve().parents[1]
    parser.add_argument(
        "--openapi",
        type=Path,
        default=root / "spec" / "contracts" / "openapi.yaml",
        help="Path to the OpenAPI YAML contract.",
    )
    parser.add_argument(
        "--schemas",
        type=Path,
        default=root / "spec" / "contracts" / "schemas",
        help="Directory with JSON Schema definitions.",
    )
    parser.add_argument(
        "--stubs-output",
        type=Path,
        default=root / "src" / "app" / "api" / "generated" / "stubs.py",
        help="Destination for generated API stubs.",
    )
    parser.add_argument(
        "--models-output",
        type=Path,
        default=root / "src" / "app" / "api" / "schemas" / "models.py",
        help="Destination for generated Pydantic models.",
    )
    args = parser.parse_args()

    with args.openapi.open("r", encoding="utf-8") as fd:
        spec = safe_load(fd) or {}
    operations = _parse_operations(spec)
    if not operations:
        raise SystemExit("No operations found in OpenAPI spec.")
    version = str(spec.get("info", {}).get("version", "unknown"))

    component_schemas: dict[str, Any] = {}
    components = spec.get("components") if isinstance(spec, Mapping) else None
    if isinstance(components, Mapping):
        raw_schemas = components.get("schemas")
        if isinstance(raw_schemas, Mapping):
            component_schemas = dict(raw_schemas)

    schema_generator = SchemaGenerator(args.schemas)
    generated_models = schema_generator.generate(components=component_schemas)

    stubs_text = render_stubs(operations, version)
    models_text = render_models(generated_models)

    args.stubs_output.parent.mkdir(parents=True, exist_ok=True)
    args.stubs_output.write_text(stubs_text, encoding="utf-8")
    args.models_output.parent.mkdir(parents=True, exist_ok=True)
    args.models_output.write_text(models_text, encoding="utf-8")


if __name__ == "__main__":
    main()
